"""
åŸç¥ç´ æçˆ¬å–å·¥å…·
- æ”¯æŒ SPA ç½‘ç«™ï¼ˆä»æºç ä¸­æ­£åˆ™æå–æ‰€æœ‰å›¾ç‰‡ URLï¼‰
- æ”¯æŒå¤šé¡µé¢æ‰¹é‡çˆ¬å–
- æ”¯æŒä» CSS/JS ä¸­æå–å›¾ç‰‡å¼•ç”¨
- æ”¯æŒæŒ‡å®šä¿å­˜ç›®å½•
- è‡ªåŠ¨è¿‡æ»¤å°å›¾/å›¾æ ‡ï¼Œåªä¿ç•™æœ‰ä»·å€¼çš„ç´ æ
"""
import os
import re
import sys
import hashlib
import requests
from urllib.parse import urljoin, urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==================== é…ç½® ====================

SAVE_DIR = os.path.join(os.path.dirname(__file__), 'frontend', 'public', 'assets')

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
                  'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/*,*/*;q=0.8',
}

# è¦çˆ¬å–çš„é¡µé¢åˆ—è¡¨
TARGET_URLS = [
    'https://ys.mihoyo.com/main/',
    'https://ys.mihoyo.com/main/character/mondstadt',
    'https://ys.mihoyo.com/main/character/liyue',
    'https://ys.mihoyo.com/main/news',
]

# é¢å¤–çš„å·²çŸ¥é«˜æ¸…ç´ æï¼ˆè¿™äº› CDN å¯ä»¥ç›´æ¥ä¸‹è½½ï¼‰
EXTRA_RESOURCES = {
    # è§’è‰²ç¥ˆæ„¿ç«‹ç»˜ (æ¥è‡ª enka.networkï¼Œå…¬å¼€å¯ç”¨)
    'venti.png':     'https://enka.network/ui/UI_Gacha_AvatarImg_Venti.png',
    'keqing.png':    'https://enka.network/ui/UI_Gacha_AvatarImg_Keqing.png',
    'zhongli.png':   'https://enka.network/ui/UI_Gacha_AvatarImg_Zhongli.png',
    'raiden.png':    'https://enka.network/ui/UI_Gacha_AvatarImg_Shougun.png',
    'nahida.png':    'https://enka.network/ui/UI_Gacha_AvatarImg_Nahida.png',
    'hutao.png':     'https://enka.network/ui/UI_Gacha_AvatarImg_Hutao.png',
    'ganyu.png':     'https://enka.network/ui/UI_Gacha_AvatarImg_Ganyu.png',
    'ayaka.png':     'https://enka.network/ui/UI_Gacha_AvatarImg_Ayaka.png',
    # è§’è‰²å¤´åƒ
    'icon_venti.png':  'https://enka.network/ui/UI_AvatarIcon_Venti.png',
    'icon_keqing.png': 'https://enka.network/ui/UI_AvatarIcon_Keqing.png',
    # é«˜æ¸…èƒŒæ™¯ï¼ˆUnsplash é£æ™¯ï¼‰
    'bg-mountain.jpg': 'https://images.unsplash.com/photo-1464822759023-fed622ff2c3b?w=1920&q=85',
    'bg-fantasy.jpg':  'https://images.unsplash.com/photo-1534796636912-3b95b3ab5986?w=1920&q=85',
    'bg-valley.jpg':   'https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=1920&q=85',
}

# å›¾ç‰‡ URL åŒ¹é…æ­£åˆ™ï¼ˆåŒ¹é… .jpg .png .webp .jpeg .svg .gifï¼‰
IMG_URL_PATTERN = re.compile(
    r'(https?://[^\s\'"<>]+?\.(?:jpg|jpeg|png|webp|gif|svg)(?:\?[^\s\'"<>]*)?)',
    re.IGNORECASE
)

# æœ€å°æ–‡ä»¶å¤§å°ï¼ˆè¿‡æ»¤å°å›¾æ ‡ï¼Œä½äºæ­¤å€¼è·³è¿‡ï¼‰
MIN_FILE_SIZE = 5 * 1024  # 5KB

# ==================== æ ¸å¿ƒé€»è¾‘ ====================

def fetch_page(url):
    """è·å–é¡µé¢æºç """
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
        return resp.text
    except Exception as e:
        print(f'  [è·³è¿‡] æ— æ³•è®¿é—® {url}: {e}')
        return ''


def extract_image_urls(html, base_url):
    """ä» HTML/JS/CSS æºç ä¸­æå–æ‰€æœ‰å›¾ç‰‡ URL"""
    urls = set()

    # 1. æ­£åˆ™åŒ¹é…æ‰€æœ‰å›¾ç‰‡ URL
    for match in IMG_URL_PATTERN.findall(html):
        full = urljoin(base_url, match)
        urls.add(full)

    # 2. åŒ¹é… url('...') ä¸­çš„è·¯å¾„
    for match in re.findall(r'url\(["\']?(.*?\.(?:jpg|jpeg|png|webp|gif))["\']?\)', html, re.I):
        full = urljoin(base_url, match)
        if full.startswith('http'):
            urls.add(full)

    # 3. åŒ¹é… src="..." data-src="..." poster="..." ç­‰å±æ€§
    for match in re.findall(r'(?:src|data-src|poster|content)=["\']([^"\']+\.(?:jpg|jpeg|png|webp|gif)[^"\']*)["\']', html, re.I):
        full = urljoin(base_url, match)
        if full.startswith('http'):
            urls.add(full)

    return urls


def extract_asset_links(html, base_url):
    """æå–é¡µé¢å¼•ç”¨çš„ CSS/JS æ–‡ä»¶ URLï¼ˆç”¨äºè¿›ä¸€æ­¥æœç´¢å›¾ç‰‡ï¼‰"""
    links = set()
    for match in re.findall(r'(?:href|src)=["\']([^"\']+\.(?:css|js))["\']', html, re.I):
        full = urljoin(base_url, match)
        if full.startswith('http'):
            links.add(full)
    return links


def download_file(url, save_path):
    """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20, stream=True)
        resp.raise_for_status()

        content = resp.content
        if len(content) < MIN_FILE_SIZE:
            return None, 'too_small'

        with open(save_path, 'wb') as f:
            f.write(content)
        return save_path, len(content)
    except Exception as e:
        return None, str(e)


def safe_filename(url, index=0):
    """ä» URL ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
    parsed = urlparse(url)
    name = os.path.basename(parsed.path)

    # æ¸…ç†æ–‡ä»¶å
    name = re.sub(r'[^\w.\-]', '_', name)

    if not name or len(name) > 60 or name.startswith('.'):
        ext = '.jpg'
        for e in ['.png', '.webp', '.gif', '.svg', '.jpeg']:
            if e in url.lower():
                ext = e
                break
        name = f'img_{index}_{hashlib.md5(url.encode()).hexdigest()[:8]}{ext}'

    return name


def crawl_page(url, all_img_urls):
    """çˆ¬å–å•ä¸ªé¡µé¢çš„æ‰€æœ‰å›¾ç‰‡ URL"""
    print(f'\nğŸ“„ çˆ¬å–é¡µé¢: {url}')
    html = fetch_page(url)
    if not html:
        return

    # ä»é¡µé¢æå–å›¾ç‰‡
    imgs = extract_image_urls(html, url)
    print(f'   æ‰¾åˆ° {len(imgs)} ä¸ªå›¾ç‰‡ URL')
    all_img_urls.update(imgs)

    # ä» CSS/JS æ–‡ä»¶ä¸­æå–æ›´å¤šå›¾ç‰‡
    asset_links = extract_asset_links(html, url)
    print(f'   æ‰¾åˆ° {len(asset_links)} ä¸ª CSS/JS æ–‡ä»¶ï¼Œæ­£åœ¨æ‰«æ...')

    for link in list(asset_links)[:20]:  # é™åˆ¶æœ€å¤šæ‰«æ20ä¸ªæ–‡ä»¶
        asset_html = fetch_page(link)
        if asset_html:
            more_imgs = extract_image_urls(asset_html, link)
            all_img_urls.update(more_imgs)


def main():
    os.makedirs(SAVE_DIR, exist_ok=True)
    print(f'ğŸ¯ ä¿å­˜ç›®å½•: {SAVE_DIR}')
    print(f'ğŸŒ ç›®æ ‡é¡µé¢: {len(TARGET_URLS)} ä¸ª')
    print(f'ğŸ¨ é¢å¤–ç´ æ: {len(EXTRA_RESOURCES)} ä¸ª')
    print('=' * 60)

    # â”€â”€ é˜¶æ®µ1: çˆ¬å–é¡µé¢å›¾ç‰‡ â”€â”€
    all_img_urls = set()
    for url in TARGET_URLS:
        crawl_page(url, all_img_urls)

    print(f'\nğŸ“Š æ€»å…±å‘ç° {len(all_img_urls)} ä¸ªä¸é‡å¤çš„å›¾ç‰‡ URL')

    # â”€â”€ é˜¶æ®µ2: ä¸‹è½½é¡µé¢å›¾ç‰‡ â”€â”€
    print('\n' + '=' * 60)
    print('â¬‡ï¸  å¼€å§‹ä¸‹è½½é¡µé¢å›¾ç‰‡...\n')

    downloaded = 0
    skipped = 0
    failed = 0

    for i, url in enumerate(sorted(all_img_urls)):
        fname = safe_filename(url, i)
        fpath = os.path.join(SAVE_DIR, fname)

        if os.path.exists(fpath):
            skipped += 1
            continue

        result, info = download_file(url, fpath)
        if result:
            size_kb = info / 1024
            print(f'  âœ… {fname} ({size_kb:.0f}KB)')
            downloaded += 1
        elif info == 'too_small':
            skipped += 1
        else:
            failed += 1

    print(f'\n  é¡µé¢å›¾ç‰‡: ä¸‹è½½ {downloaded}, è·³è¿‡ {skipped}, å¤±è´¥ {failed}')

    # â”€â”€ é˜¶æ®µ3: ä¸‹è½½é¢å¤–é«˜æ¸…ç´ æ â”€â”€
    print('\n' + '=' * 60)
    print('ğŸ¨ ä¸‹è½½é¢å¤–é«˜æ¸…ç´ æ...\n')

    for fname, url in EXTRA_RESOURCES.items():
        fpath = os.path.join(SAVE_DIR, fname)
        if os.path.exists(fpath) and os.path.getsize(fpath) > MIN_FILE_SIZE:
            print(f'  â­ï¸  {fname} (å·²å­˜åœ¨)')
            continue

        result, info = download_file(url, fpath)
        if result:
            size_kb = info / 1024
            print(f'  âœ… {fname} ({size_kb:.0f}KB)')
        else:
            print(f'  âŒ {fname}: {info}')

    # â”€â”€ å®Œæˆ â”€â”€
    print('\n' + '=' * 60)
    all_files = os.listdir(SAVE_DIR)
    total_size = sum(os.path.getsize(os.path.join(SAVE_DIR, f)) for f in all_files)
    print(f'ğŸ å®Œæˆï¼å…± {len(all_files)} ä¸ªæ–‡ä»¶ï¼Œæ€»å¤§å° {total_size/1024/1024:.1f}MB')
    print(f'ğŸ“ {SAVE_DIR}')


if __name__ == '__main__':
    main()
